{% set name = "tokenizers" %}
{% set version = "0.11.4" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  git_url: https://github.com/huggingface/tokenizers.git
  git_rev: python-v{{ version }}
  #patches:
    #- 0301-update-lexical-core-because-0.7.4-doesnot-compile.patch

build:
  number: 1

requirements:
  build:
    - {{ compiler('cxx') }}
    - {{ compiler('rust') }}
  host:
    - pip {{ pip }}
    - python {{ python }}
    - setuptools-rust {{ setuptools_rust }}
    - rust {{ rust }}
  run:
    - python {{ python }}

test:
  imports:
    - tokenizers
    - tokenizers.models
    - tokenizers.decoders
    - tokenizers.normalizers
    - tokenizers.pre_tokenizers
    - tokenizers.processors
    - tokenizers.trainers
    - tokenizers.implementations
  commands:
    - python -c "from tokenizers import BertWordPieceTokenizer"
    - python -c "from tokenizers import ByteLevelBPETokenizer"
    - python -c "from tokenizers import CharBPETokenizer"
    - python -c "from tokenizers import SentencePieceBPETokenizer"

about:
  home: https://github.com/huggingface/tokenizers
  license: Apache-2.0
  license_family: APACHE
  license_file: LICENSE
  summary: Fast State-of-the-Art Tokenizers optimized for Research and Production
  description: Fast State-of-the-Art Tokenizers optimized for Research and Production
  doc_url: https://huggingface.co/tokenizers/

extra:
  recipe-maintainers:
    - open-ce/open-ce-dev-team
